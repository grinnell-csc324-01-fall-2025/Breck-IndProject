Research Questions:
  1. Does natural language explanation of chess moves lead to a better understanding
     and move quality improvement compared to traditional numerical engine evaluations?
  2. Does dynamically adjusting explanation complexity based on demonstrated user understanding 
     improve learning efficiency?
  3. Does having a ladder system of AI skill level cause more improvements vs a dynamically changing level of skill?

Experiment Questions:
  1.
    1. Recall Question: "In the position around move 4, it suggested this movement. Can you explain in your own words why that move was good?"

    2. Application Question: "Looking at this new position, what would be a good move and why?"
    3. Pattern Recognition: "Did you notice any recurring themes or ideas in the positions we looked at today?"
    4. Confidence Rating: "On a scale of 1-7, how confident are you that you understood the AI's explanations?"
    5. Confusion Check: "Was there any point where the AI's explanation confused you or seemed unclear?"
    6. Actionability: "How easy was it to apply the AI's advice to your next moves?"
    7. Learning Perception: "Do you feel like you learned something about chess from this session?" 
    8. Explanation Quality: "Did the AI explain the 'why' behind moves, or just tell you what to do?"
    9. Preference: "Would you want to use this style of AI coaching in your regular chess practice?"

  2.
    1. Difficulty Perception: "On a scale of 1-7, how would you rate the difficulty level of the AI's explanations?" 
    2. Cognitive Load: "How mentally demanding was it to understand and process the AI's feedback?"
    3. Engagement Check: "Did you ever feel bored or talked down to during the explanations?"
    4. Overwhelm Check: "Did you ever feel lost or overwhelmed by the complexity of what the AI was explaining?"
    5. Learning Efficiency: "How much do you feel you learned in this short session?"
    6. Concept Application: "Show me in this position [present test position] - can you apply something you learned today?"
    7. Personalization Awareness (for adaptive group): "Did you notice the AI adjusting its explanations as we went along?"
    8. Appropriateness: "Did the explanations feel like they were at the right level for your chess understanding?"
    9. Frustration Check: "Rate your frustration level during this session."
  3. 
    1. Challenge Rating: "How challenging was the AI opponent?"
    2. Progress Awareness: "Did you feel like you were improving or making progress during the session?"
    3. Motivation: "How motivated did you feel to keep playing and improving?"
    4. Level Understanding (Ladder only): "What level did you reach? How did that make you feel?"
    5. Adjustment Awareness (Dynamic only): "Did you notice the AI's playing strength changing?"
    6. Goal Clarity: "Did you have a clear sense of what you were working toward?"
    7. Satisfaction: "How satisfied are you with your performance in this session?"
    8. Fairness Perception: "Did the AI's skill level feel fair and appropriate for your ability?"
    9. Preference: "Would you prefer to see explicit levels (Bronze, Silver, Gold) or have the AI invisibly adjust to you?"
    10. Engagement Driver: "What motivated you more during the games - trying to beat the level, or just playing good chess?"
